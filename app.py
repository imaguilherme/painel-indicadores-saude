# app.py ‚Äî Perfil dos Pacientes (2019‚Äì2025) | suporta Parquet √∫nico OU 3 CSVs grandes (DuckDB)
# Requisitos: streamlit, pandas, numpy, plotly, python-dateutil, pyarrow, duckdb

import streamlit as st
import pandas as pd
import numpy as np
from dateutil import parser
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
import duckdb, os, tempfile

st.set_page_config(page_title="Perfil dos Pacientes ‚Äì ACC", layout="wide")

# -------------------- utilidades --------------------
def _to_dt(s):
    if pd.isna(s): return pd.NaT
    if isinstance(s, (pd.Timestamp, datetime)): return pd.to_datetime(s)
    try: return pd.to_datetime(parser.parse(str(s), dayfirst=True, fuzzy=True))
    except: return pd.NaT

def _post_load(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [c.lower() for c in df.columns]

    # datas
    for c in [
        "data_internacao","data_alta","data_obito",
        "dthr_valida","dt_entrada_cti","dt_saida_cti",
        "data_cirurgia_min","data_cirurgia_max"
    ]:
        if c in df.columns:
            df[c] = pd.to_datetime(df[c], errors="coerce")

    # num√©ricos
    for c in ["idade","ano","ano_internacao"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")

    # sexo
    if "sexo" in df.columns:
        df["sexo"] = (
            df["sexo"].astype(str).str.strip().str.upper()
              .replace({"M":"Masculino","F":"Feminino",
                        "MASCULINO":"Masculino","FEMININO":"Feminino"})
        )

    # derivar ano da interna√ß√£o, se n√£o vier pronto
    if "data_internacao" in df.columns and "ano_internacao" not in df.columns:
        df["ano_internacao"] = df["data_internacao"].dt.year

    # dias perman√™ncia
    if {"data_internacao","data_alta"}.issubset(df.columns):
        df["dias_permanencia"] = (df["data_alta"] - df["data_internacao"]).dt.days

    # faixas et√°rias
    if "idade" in df.columns:
        bins = [-1,0,4,11,17,24,34,44,54,64,74,84,120]
        labels = ["<1","1‚Äì4","5‚Äì11","12‚Äì17","18‚Äì24",
                  "25‚Äì34","35‚Äì44","45‚Äì54","55‚Äì64",
                  "65‚Äì74","75‚Äì84","85+"]
        df["faixa_etaria"] = pd.cut(
            pd.to_numeric(df["idade"], errors="coerce"),
            bins=bins, labels=labels, right=True
        )

    # dedup de eventos (AIH)
    keys = [c for c in ["codigo_internacao","prontuario_anonimo",
                         "data_internacao","data_alta"] if c in df.columns]
    if keys:
        df = df.drop_duplicates(subset=keys)

    return df

@st.cache_data(show_spinner=False)
def load_parquet(file):
    return _post_load(pd.read_parquet(file))

@st.cache_resource(show_spinner=False)
def load_duckdb(csv_paths):
    """
    Registra 3 CSVs (EVOLUCOES, PROCED, CIDS/UTI) e cria view dataset unificada (lazy).

    Ajustes:
    - Usa read_csv_auto apenas com o caminho (sem par√¢metros nomeados),
      para evitar InvalidInputException na vers√£o do DuckDB do Streamlit.
    - Normaliza PRONTUARIO_ANONIMO em todas as views.
    - Procedimentos agregados por PRONTUARIO_ANONIMO.
    """
    con = duckdb.connect(database=":memory:")
    evo, proc, cti = csv_paths

    def make_view(view_name: str, path: str):
        # Escapa aspas simples para n√£o quebrar o SQL
        path_esc = str(path).replace("'", "''")
        sql = f"""
            CREATE VIEW {view_name} AS
            SELECT *
            FROM read_csv_auto('{path_esc}');
        """
        con.execute(sql)

    # Cria as 3 views a partir dos CSVs
    make_view("evolu", evo)
    make_view("proced", proc)
    make_view("cids", cti)

    # normaliza chaves para lower/trim
    con.execute("""
        CREATE VIEW evolu_n AS
        SELECT
          lower(trim(CAST(PRONTUARIO_ANONIMO AS VARCHAR))) AS prontuario_anonimo,
          *
        FROM evolu;

        CREATE VIEW cids_n AS
        SELECT
          lower(trim(CAST(PRONTUARIO_ANONIMO AS VARCHAR))) AS prontuario_anonimo,
          *
        FROM cids;

        CREATE VIEW proc_n AS
        SELECT
          lower(trim(CAST(PRONTUARIO_ANONIMO AS VARCHAR))) AS prontuario_anonimo,
          *
        FROM proced;
    """)

    # Procedimentos agregados POR PRONTUARIO_ANONIMO (chave comum)
    con.execute("""
        CREATE VIEW proc_agg AS
        SELECT
          prontuario_anonimo,
          COUNT(DISTINCT CODIGO_PROCEDIMENTO)           AS n_proced,
          ANY_VALUE(CODIGO_PROCEDIMENTO)                AS proc_prim,
          ANY_VALUE(PROCEDIMENTO)                       AS proc_nome_prim,
          MIN(COALESCE(DATA_CIRURGIA, DATA_INTERNACAO)) AS data_cirurgia_min,
          MAX(COALESCE(DATA_CIRURGIA, DATA_INTERNACAO)) AS data_cirurgia_max
        FROM proc_n
        GROUP BY prontuario_anonimo;
    """)

    # Dataset final:
    # - EVOLU + CIDS: juntando por prontuario_anonimo
    # - PROCED: anexado por prontuario_anonimo
    con.execute("""
        CREATE VIEW dataset AS
        SELECT
          e.*,
          c.* EXCLUDE (prontuario_anonimo),
          p.*
        FROM evolu_n e
        LEFT JOIN cids_n  c USING (prontuario_anonimo)
        LEFT JOIN proc_agg p USING (prontuario_anonimo);
    """)

    return con



def df_from_duckdb(con, sql):
    return con.execute(sql).df()

def enrich_with_aux_tables(df: pd.DataFrame,
                           cid_file=None,
                           sigtap_file=None,
                           geo_file=None) -> pd.DataFrame:
    """
    Enriquecimento opcional com:
    - CID-10 (cap√≠tulo / grupo / subcategoria)
    - Procedimentos SIGTAP (grupo / subgrupo / forma organiza√ß√£o)
    - Tabela geogr√°fica (UF / macro / regi√£o de sa√∫de)

    Todos s√£o opcionais; se o arquivo n√£o for enviado, nada quebra.
    """
    if df is None:
        return df

    df_enriched = df.copy()

    # -------- CID-10 --------
    if cid_file is not None:
        try:
            cid_df = pd.read_csv(cid_file, dtype=str)
            cid_df.columns = [c.lower() for c in cid_df.columns]

            # tenta identificar coluna de c√≥digo CID
            cid_code_col = next(
                (c for c in cid_df.columns if "cid" in c and "descricao" not in c),
                None
            )
            if cid_code_col and "cid" in df_enriched.columns:
                df_enriched["cid"] = (
                    df_enriched["cid"].astype(str).str.strip().str.upper().str[:3]
                )
                cid_df[cid_code_col] = (
                    cid_df[cid_code_col].astype(str).str.strip().str.upper().str[:3]
                )

                keep_cols = [cid_code_col]
                keep_cols += [
                    c for c in cid_df.columns
                    if any(k in c for k in ["capit", "grupo", "subcat", "subcategoria", "desc"])
                ]
                cid_small = cid_df[keep_cols].drop_duplicates(subset=[cid_code_col])

                df_enriched = df_enriched.merge(
                    cid_small,
                    how="left",
                    left_on="cid",
                    right_on=cid_code_col
                )
        except Exception as e:
            st.warning(f"N√£o foi poss√≠vel enriquecer com CID-10: {e}")

    # -------- Procedimentos (SIGTAP) --------
    if sigtap_file is not None:
        try:
            sig_df = pd.read_csv(sigtap_file, dtype=str)
            sig_df.columns = [c.lower() for c in sig_df.columns]

            sig_code_col = next(
                (c for c in sig_df.columns
                 if "proced" in c and ("cod" in c or "codigo" in c)),
                None
            )

            proc_col = None
            for cand in ["proc_prim", "codigo_procedimento", "cod_procedimento"]:
                if cand in df_enriched.columns:
                    proc_col = cand
                    break

            if sig_code_col and proc_col:
                df_enriched[proc_col] = df_enriched[proc_col].astype(str).str.strip()
                sig_df[sig_code_col] = sig_df[sig_code_col].astype(str).str.strip()

                keep_cols = [sig_code_col]
                keep_cols += [
                    c for c in sig_df.columns
                    if any(k in c for k in ["grupo", "subgrupo", "forma", "nome"])
                ]
                sig_small = sig_df[keep_cols].drop_duplicates(subset=[sig_code_col])

                df_enriched = df_enriched.merge(
                    sig_small,
                    how="left",
                    left_on=proc_col,
                    right_on=sig_code_col
                )
        except Exception as e:
            st.warning(f"N√£o foi poss√≠vel enriquecer com SIGTAP: {e}")

       # -------- Geografia (UF / Macro / Regi√£o de Sa√∫de) --------
    if geo_file is not None:
        try:
            geo_df = pd.read_csv(geo_file, dtype=str)
            geo_df.columns = [c.lower() for c in geo_df.columns]

            # esperamos colunas: sg_uf, no_macrorregional, no_cir_padrao, no_municipio
            if "cidade_moradia" in df_enriched.columns and \
               {"no_municipio", "sg_uf"}.issubset(geo_df.columns):

                # CIDADE_MORADIA est√° no formato "cidade, UF"
                partes = df_enriched["cidade_moradia"].astype(str).str.split(",", n=1, expand=True)
                df_enriched["cidade_nome_norm"] = partes[0].str.upper().str.strip()
                if partes.shape[1] > 1:
                    df_enriched["uf_from_cidade"] = partes[1].str.upper().str.strip()
                else:
                    df_enriched["uf_from_cidade"] = np.nan

                geo_df["no_municipio_norm"] = (
                    geo_df["no_municipio"].astype(str).str.upper().str.strip()
                )
                geo_df["sg_uf"] = geo_df["sg_uf"].astype(str).str.upper().str.strip()

                # reduz para as colunas que vamos usar
                geo_small = geo_df[[
                    "no_municipio_norm", "sg_uf",
                    "no_macrorregional", "no_cir_padrao"
                ]].drop_duplicates(subset=["no_municipio_norm", "sg_uf"])

                # üîê merge por CIDADE + UF
                df_enriched = df_enriched.merge(
                    geo_small,
                    how="left",
                    left_on=["cidade_nome_norm", "uf_from_cidade"],
                    right_on=["no_municipio_norm", "sg_uf"]
                )

                # renomeia para nomes mais amig√°veis no painel
                df_enriched = df_enriched.rename(columns={
                    "sg_uf": "uf",
                    "no_macrorregional": "macroregiao",
                    "no_cir_padrao": "regiao_saude",
                })

                # limpa colunas auxiliares
                df_enriched = df_enriched.drop(
                    columns=["cidade_nome_norm", "no_municipio_norm"],
                    errors="ignore"
                )

        except Exception as e:
            st.warning(f"N√£o foi poss√≠vel enriquecer com regi√µes de sa√∫de: {e}")

    # üîö ESSA LINHA √â FUNDAMENTAL
    return df_enriched

# -------------------- fun√ß√µes de base --------------------
def pacientes_unicos(df: pd.DataFrame) -> pd.DataFrame:
    if {"prontuario_anonimo","data_internacao"}.issubset(df.columns):
        return (
            df.sort_values(["prontuario_anonimo","data_internacao"])
              .groupby("prontuario_anonimo", as_index=False)
              .tail(1)
        )
    if "prontuario_anonimo" in df.columns:
        return df.drop_duplicates(subset=["prontuario_anonimo"])
    return df

def kpis(df_eventos: pd.DataFrame, df_pacientes: pd.DataFrame):
    pacientes = (
        df_pacientes["prontuario_anonimo"].nunique()
        if "prontuario_anonimo" in df_pacientes else np.nan
    )
    internacoes = (
        df_eventos["codigo_internacao"].nunique()
        if "codigo_internacao" in df_eventos else len(df_eventos)
    )
    tmi = (
        df_eventos["dias_permanencia"]
          .replace([np.inf,-np.inf], np.nan)
          .dropna().mean()
        if "dias_permanencia" in df_eventos else np.nan
    )

    mort_hosp = np.nan
    if {"data_internacao","data_alta"}.issubset(df_eventos.columns):
        e = df_eventos.copy()
        if "data_obito" in e.columns:
            e["obito_no_periodo"] = (
                (e["data_obito"].notna()) &
                (e["data_obito"] >= e["data_internacao"]) &
                (e["data_obito"] <= (e["data_alta"] - pd.Timedelta(days=1)))
            )
        elif "evolucao" in e.columns:
            e["obito_no_periodo"] = e["evolucao"].astype(str).str.contains(
                "√ìBITO", case=False, na=False
            )
        else:
            e["obito_no_periodo"] = False

        denom = (
            e["codigo_internacao"].nunique()
            if "codigo_internacao" in e else len(e)
        )
        numer = e.loc[e["obito_no_periodo"]]
        numer = (
            numer["codigo_internacao"].nunique()
            if "codigo_internacao" in e else len(numer)
        )
        mort_hosp = (numer/denom*100) if denom else np.nan

    return pacientes, internacoes, tmi, mort_hosp

def reinternacao_30d_pos_proced(df: pd.DataFrame):
    ok = {"prontuario_anonimo","codigo_internacao",
          "data_internacao","data_alta"}.issubset(df.columns)
    if not ok: return np.nan
    s = df.sort_values(
        ["prontuario_anonimo","data_internacao","data_alta"]
    ).copy()
    s["next_dt_internacao"] = (
        s.groupby("prontuario_anonimo")["data_internacao"].shift(-1)
    )
    s["delta_proc"] = (
        s["next_dt_internacao"] - s["data_internacao"]
    ).dt.days
    s["delta_pos_alta"] = (
        s["next_dt_internacao"] - s["data_alta"]
    ).dt.days
    s["transfer"] = s["delta_pos_alta"] <= 1
    base = s["codigo_internacao"].nunique()
    numer = s[
        s["delta_proc"].between(0,30, inclusive="both") & (~s["transfer"])
    ]["codigo_internacao"].nunique()
    return (numer/base*100) if base else np.nan

def reinternacao_30d_pos_alta(df: pd.DataFrame):
    ok = {"prontuario_anonimo","codigo_internacao",
          "data_internacao","data_alta"}.issubset(df.columns)
    if not ok: return np.nan
    s = df.sort_values(
        ["prontuario_anonimo","data_internacao","data_alta"]
    ).copy()
    s["next_dt_internacao"] = (
        s.groupby("prontuario_anonimo")["data_internacao"].shift(-1)
    )
    s["delta"] = (s["next_dt_internacao"] - s["data_alta"]).dt.days
    s["transfer"] = s["delta"] <= 1
    base = s["codigo_internacao"].nunique()
    numer = s[
        s["delta"].between(0,30, inclusive="both") & (~s["transfer"])
    ]["codigo_internacao"].nunique()
    return (numer/base*100) if base else np.nan

# -------------------- filtros e indicadores --------------------
def build_filters(df: pd.DataFrame):
    st.sidebar.header("Filtros")

    # ano
    anos_col = (
        "ano_internacao" if "ano_internacao" in df.columns
        else ("ano" if "ano" in df.columns else None)
    )
    if anos_col:
        anos = sorted(df[anos_col].dropna().unique().tolist())
    else:
        anos = []
    ano_sel = st.sidebar.multiselect("Ano da interna√ß√£o", anos, default=anos)

    # idade
    if "idade" in df.columns and df["idade"].notna().any():
        idade_min, idade_max = int(np.nanmin(df["idade"])), int(np.nanmax(df["idade"]))
    else:
        idade_min, idade_max = 0, 120
    idade_sel = st.sidebar.slider(
        "Idade", min_value=0, max_value=max(idade_max,1),
        value=(idade_min, idade_max), step=1
    )

    # estado (se existir)
    estado_col = next(
        (c for c in df.columns if c.lower() in
         ["estado_residencia","uf_residencia","uf","estado","sigla_uf"]),
        None
    )
    estados_sel = []
    if estado_col:
        estados = sorted(df[estado_col].dropna().astype(str).unique().tolist())
        estados_sel = st.sidebar.multiselect(
            "Estado de resid√™ncia", estados, default=estados
        )

    # regi√£o de sa√∫de (se existir)
    regiao_col = next(
        (c for c in df.columns
         if "regiao" in c.lower() and "saude" in c.lower()),
        None
    )
    regioes_sel = []
    if regiao_col:
        regioes = sorted(df[regiao_col].dropna().astype(str).unique().tolist())
        regioes_sel = st.sidebar.multiselect(
            "Regi√£o de sa√∫de", regioes, default=regioes
        )

    # munic√≠pio de resid√™ncia (lista baseada em pacientes √∫nicos)
    df_pac_ref = pacientes_unicos(df)
    cidade_col = "cidade_moradia" if "cidade_moradia" in df_pac_ref.columns else None
    cidades_sel = []
    if cidade_col:
        cidade_vals = sorted(
            df_pac_ref[cidade_col].dropna().astype(str).unique().tolist()
        )
        default_cidades = (
            cidade_vals if len(cidade_vals) <= 25 else cidade_vals[:25]
        )
        cidades_sel = st.sidebar.multiselect(
            "Munic√≠pio de resid√™ncia", cidade_vals, default=default_cidades
        )

    return {
        "ano": ano_sel,
        "idade": idade_sel,
        "estado": estados_sel,
        "regiao": regioes_sel,
        "cidade": cidades_sel,
    }

def apply_filters(df, f):
    # ano
    if "ano_internacao" in df.columns and f["ano"]:
        df = df[df["ano_internacao"].isin(f["ano"])]
    elif "ano" in df.columns and f["ano"]:
        df = df[df["ano"].isin(f["ano"])]

    # idade
    if "idade" in df.columns and f["idade"]:
        df = df[
            (df["idade"] >= f["idade"][0]) &
            (df["idade"] <= f["idade"][1])
        ]

    # estado
    estado_col = next(
        (c for c in df.columns if c.lower() in
         ["estado_residencia","uf_residencia","uf","estado","sigla_uf"]),
        None
    )
    if estado_col and f["estado"]:
        df = df[df[estado_col].isin(f["estado"])]

    # regi√£o de sa√∫de
    regiao_col = next(
        (c for c in df.columns
         if "regiao" in c.lower() and "saude" in c.lower()),
        None
    )
    if regiao_col and f["regiao"]:
        df = df[df[regiao_col].isin(f["regiao"])]

    # munic√≠pio
    if "cidade_moradia" in df.columns and f["cidade"]:
        df = df[df["cidade_moradia"].isin(f["cidade"])]

    return df

def show_active_filters(f):
    partes = []
    if f["ano"]:
        partes.append("**Ano:** " + ", ".join(str(a) for a in f["ano"]))
    if f["idade"]:
        partes.append(f"**Idade:** {f['idade'][0]}‚Äì{f['idade'][1]} anos")
    if f["estado"]:
        partes.append("**Estado:** " + ", ".join(f["estado"]))
    if f["regiao"]:
        partes.append("**Regi√£o de sa√∫de:** " + ", ".join(f["regiao"]))
    if f["cidade"]:
        partes.append("**Munic√≠pio:** " + ", ".join(f["cidade"]))

    if partes:
        st.markdown("**Filtros ativos:** " + " | ".join(partes))
    else:
        st.markdown("**Filtros ativos:** nenhum filtro aplicado.")

# -------------------- UI: carregamento --------------------
st.title("Perfil dos Pacientes")

tab_parquet, tab_csv = st.tabs(["Parquet √∫nico (recomendado)", "3 CSVs grandes (DuckDB)"])

df = None
with tab_parquet:
    file_parquet = st.file_uploader(
        "Carregue o Parquet √∫nico (dataset_unico_2019_2025.parquet)",
        type=["parquet"], key="pq"
    )
    if file_parquet:
        df = load_parquet(file_parquet)

with tab_csv:
    c1, c2, c3 = st.columns(3)
    evo = c1.file_uploader("EVOLU√á√ïES (csv)", type=["csv"], key="evo")
    proc = c2.file_uploader("PROCEDIMENTOS (csv)", type=["csv"], key="proc")
    cti = c3.file_uploader("CIDs/UTI (csv)", type=["csv"], key="cti")
    if evo and proc and cti:
        tmpdir = tempfile.mkdtemp()
        p_evo = os.path.join(tmpdir, "evo.csv");  open(p_evo,  "wb").write(evo.getbuffer())
        p_proc = os.path.join(tmpdir, "proc.csv"); open(p_proc, "wb").write(proc.getbuffer())
        p_cti = os.path.join(tmpdir, "cti.csv");  open(p_cti,  "wb").write(cti.getbuffer())
        con = load_duckdb((p_evo, p_proc, p_cti))
        df = df_from_duckdb(con, "SELECT * FROM dataset")
        df = _post_load(df)

if df is None or df.empty:
    st.info("Carregue um Parquet √∫nico **ou** os 3 CSVs.")
    st.stop()

# -------- uploads das tabelas auxiliares --------
with st.expander("Carregar tabelas auxiliares (opcional) ‚Äì CID-10, SIGTAP e Regi√µes de Sa√∫de"):
    cid_file = st.file_uploader(
        "Tabela de CIDs (classifica√ß√£o em cap√≠tulo/grupo/subcategoria) ‚Äì CSV",
        type=["csv"], key="cid_map"
    )
    sigtap_file = st.file_uploader(
        "Tabela de procedimentos (SIGTAP) ‚Äì CSV",
        type=["csv"], key="sigtap_map"
    )
    geo_file = st.file_uploader(
        "Tabela de Regi√µes/Macrorregi√µes de Sa√∫de e Munic√≠pios ‚Äì CSV",
        type=["csv"], key="geo_map"
    )

if cid_file or sigtap_file or geo_file:
    df = enrich_with_aux_tables(df, cid_file, sigtap_file, geo_file)

# -------------------- filtros e bases --------------------
f = build_filters(df)
df_f = apply_filters(df, f)          # eventos (AIHs) filtrados
df_pac = pacientes_unicos(df_f)     # 1 linha por paciente filtrado

# mostra filtros ativos no corpo da p√°gina
show_active_filters(f)
st.divider()

# toggle de an√°lise
modo_perfil = st.toggle(
    "Contar por **paciente √∫nico** (perfil). Desative para **interna√ß√µes**.",
    value=True
)
base = df_pac if modo_perfil else df_f

# -------------------- KPIs --------------------
pacientes, internacoes, tmi, mort_hosp = kpis(df_f, df_pac)
ri_proc = reinternacao_30d_pos_proced(df_f)
ri_alta = reinternacao_30d_pos_alta(df_f)

k1,k2,k3,k4,k5,k6 = st.columns(6)
k1.metric("Pacientes (distintos)",
          f"{int(pacientes):,}".replace(",",".") if pd.notna(pacientes) else "‚Äî")
k2.metric("Interna√ß√µes",
          f"{int(internacoes):,}".replace(",",".") if pd.notna(internacoes) else "‚Äî")
k3.metric("Tempo m√©dio de interna√ß√£o (dias)",
          f"{tmi:.1f}" if pd.notna(tmi) else "‚Äî")
k4.metric("Reinterna√ß√£o 30d (procedimento)",
          f"{ri_proc:.1f}%" if pd.notna(ri_proc) else "‚Äî")
k5.metric("Reinterna√ß√£o 30d (alta)",
          f"{ri_alta:.1f}%" if pd.notna(ri_alta) else "‚Äî")
k6.metric("Mortalidade hospitalar",
          f"{mort_hosp:.1f}%" if pd.notna(mort_hosp) else "‚Äî")

st.divider()

# -------------------- "Abas" de indicador + comparativo anual --------------------
st.markdown("### Indicadores principais")

indicador_top = st.radio(
    "Selecione o indicador para o comparativo anual:",
    ["Quantidade de pacientes", "Quantidade de interna√ß√µes"],
    horizontal=True,
    key="ind_top"
)

ano_col = (
    "ano_internacao" if "ano_internacao" in df_f.columns
    else ("ano" if "ano" in df_f.columns else None)
)

if ano_col:
    df_year = df_f[~df_f[ano_col].isna()].copy()
    if not df_year.empty:
        grp = df_year.groupby(ano_col)

        if indicador_top == "Quantidade de pacientes":
            if "prontuario_anonimo" in df_year.columns:
                serie = grp["prontuario_anonimo"].nunique()
            else:
                serie = grp.size()
            y_label = "Pacientes distintos"
        else:  # Quantidade de interna√ß√µes
            if "codigo_internacao" in df_year.columns:
                serie = grp["codigo_internacao"].nunique()
            else:
                serie = grp.size()
            y_label = "Interna√ß√µes"

        df_plot = serie.reset_index(name="valor").sort_values(ano_col)
        fig_ano = px.bar(df_plot, x=ano_col, y="valor", text_auto=True)
        fig_ano.update_layout(
            xaxis_title="Ano",
            yaxis_title=y_label,
            height=280,
            margin=dict(t=40, b=40)
        )
        st.plotly_chart(fig_ano, use_container_width=True)
    else:
        st.info("Sem dados para o comparativo anual com os filtros atuais.")
else:
    st.info("Coluna de ano n√£o encontrada no dataset.")

st.divider()

# -------------------- GRID PRINCIPAL--------------------
col_esq, col_meio, col_dir = st.columns([1.1, 1.3, 1.1])

# ========= COLUNA ESQUERDA =========
with col_esq:
    # linha Sexo + Car√°ter de Atendimento
    c1, c2 = st.columns(2)

    with c1:
        st.subheader("Sexo")
        if "sexo" in base.columns:
            df_sexo = base.value_counts("sexo").rename("cont").reset_index()
            fig = px.bar(df_sexo, x="sexo", y="cont", text_auto=True)
            fig.update_layout(
                height=230,
                margin=dict(t=40, b=30)
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("Coluna 'sexo' n√£o encontrada.")

    with c2:
        # Car√°ter de atendimento (se existir)
        carater_col = None
        for cand in [
            "carater_atendimento","caracter_atendimento","carater",
            "car√°ter_atendimento","carater_atend"
        ]:
            if cand in df_f.columns:
                carater_col = cand
                break

        st.subheader("Car√°ter do atendimento")
        if carater_col:
            ordem = df_f[carater_col].value_counts().index.tolist()
            df_car = df_f.value_counts(carater_col).rename("cont").reset_index()
            fig = px.bar(
                df_car, x=carater_col, y="cont", text_auto=True,
                category_orders={carater_col: ordem}
            )
            fig.update_layout(
                height=230,
                xaxis_title="",
                margin=dict(t=40, b=80)
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("Coluna de 'car√°ter do atendimento' n√£o encontrada.")

    # Pir√¢mide et√°ria
    st.subheader("Pir√¢mide et√°ria")
    if {"idade","sexo","faixa_etaria"}.issubset(base.columns):
        tmp = (
            base.dropna(subset=["faixa_etaria","sexo"])
                .groupby(["faixa_etaria","sexo"])
                .size().reset_index(name="n")
        )
        male = tmp[tmp["sexo"].eq("Masculino")].set_index("faixa_etaria")["n"].reindex(
            base["faixa_etaria"].cat.categories, fill_value=0
        )
        female = tmp[tmp["sexo"].eq("Feminino")].set_index("faixa_etaria")["n"].reindex(
            base["faixa_etaria"].cat.categories, fill_value=0
        )
        fig = go.Figure()
        fig.add_bar(
            y=male.index.astype(str), x=-male.values,
            name="Masculino", orientation="h"
        )
        fig.add_bar(
            y=female.index.astype(str), x=female.values,
            name="Feminino", orientation="h"
        )
        fig.update_layout(
            barmode="overlay",
            xaxis=dict(title="Pacientes (neg=M)"),
            yaxis=dict(title="Faixa et√°ria"),
            height=380,
            margin=dict(t=40, b=40)
        )
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Requer colunas 'idade', 'sexo' e 'faixa_etaria'.")

# ========= COLUNA DO MEIO =========
with col_meio:
    st.subheader("Estado ‚Üí Regi√£o de Sa√∫de ‚Üí Munic√≠pio de resid√™ncia")

    if {"uf", "regiao_saude", "cidade_moradia"}.issubset(base.columns):
        df_geo_plot = base.dropna(subset=["cidade_moradia"]).copy()
        df_geo_plot["Pacientes/Interna√ß√µes"] = 1

        fig = px.treemap(
            df_geo_plot,
            path=["uf", "regiao_saude", "cidade_moradia"],
            values="Pacientes/Interna√ß√µes",
        )
        fig.update_layout(
            height=550,
            margin=dict(t=40, l=0, r=0, b=0)
        )
        st.plotly_chart(fig, use_container_width=True)
        st.caption(
            "Hierarquia: Estado ‚Üí Regi√£o de Sa√∫de ‚Üí Munic√≠pio. "
            "Refine usando os filtros de Estado / Regi√£o de Sa√∫de / Munic√≠pio."
        )
    else:
        st.error(
            "Colunas 'uf', 'regiao_saude' ou 'cidade_moradia' n√£o est√£o dispon√≠veis.\n\n"
            "Verifique se o arquivo de Regi√µes/Macrorregi√µes de Sa√∫de foi carregado no painel "
            "e se o enriquecimento geogr√°fico est√° habilitado."
        )

    # Ra√ßa √ó Sexo
    st.subheader("Ra√ßa/Cor √ó Sexo")
    if {"etnia","sexo"}.issubset(base.columns):
        df_etnia = (
            base.value_counts(["etnia","sexo"])
                .rename("cont").reset_index()
        )
        fig = px.bar(
            df_etnia, x="etnia", y="cont", color="sexo",
            barmode="group", text_auto=True
        )
        fig.update_layout(
            xaxis_title="Ra√ßa/Cor",
            yaxis_title="Pacientes/Interna√ß√µes",
            height=320,
            margin=dict(t=40, b=80)
        )
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Requer colunas 'etnia' e 'sexo'.")

# ========= COLUNA DIREITA =========
with col_dir:
    # Card grande de quantidade de pacientes
    st.subheader("Quantidade de pacientes")
    st.markdown(
        f"<h2 style='text-align:center;'>{int(pacientes):,}</h2>".replace(",","."),
        unsafe_allow_html=True
    )
    st.caption("Pacientes distintos no per√≠odo filtrado")

    st.markdown("---")

    # Procedimentos (top N)
    st.subheader("Procedimentos (amostra)")
    proc_cols = [
        c for c in base.columns
        if "proc_nome_prim" in c or c.lower() == "procedimento"
    ]
    if proc_cols:
        pcol = proc_cols[0]
        top_proc = (
            base[pcol].dropna().astype(str)
                .value_counts().head(10).reset_index()
        )
        top_proc.columns = ["Procedimento", "Pacientes/Interna√ß√µes"]
        fig = px.bar(
            top_proc, x="Procedimento", y="Pacientes/Interna√ß√µes", text_auto=True
        )
        fig.update_layout(
            xaxis_tickangle=-35,
            height=260,
            margin=dict(t=40, b=120)
        )
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("N√£o encontrei coluna de procedimento agregada.")

    st.markdown("---")

    # Grupo / categorias CID-10
    st.subheader("Grupo e categorias de CID-10 (amostra)")
    cid_col = [
        c for c in df_f.columns
        if ("cid" in c.lower() or "descricao" in c.lower()
            or "to_charsubstrievdescricao14000" in c.lower())
    ]
    if cid_col:
        col_cid = cid_col[0]
        top = (
            df_f[col_cid].dropna().astype(str).str.upper().str[:50]
                .value_counts().head(10).reset_index()
        )
        top.columns = ["CID/Descri√ß√£o (amostra)", "Frequ√™ncia"]
        fig = px.bar(
            top, x="CID/Descri√ß√£o (amostra)", y="Frequ√™ncia", text_auto=True
        )
        fig.update_layout(
            xaxis_tickangle=-35,
            height=260,
            margin=dict(t=40, b=120)
        )
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("N√£o encontrei coluna de CID/descri√ß√£o no dataset.")
